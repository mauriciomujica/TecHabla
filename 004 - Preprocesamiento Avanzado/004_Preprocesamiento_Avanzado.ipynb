{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cuaderno Clase PLN - Preprocesamiento Avanzado\n"
      ],
      "metadata": {
        "id": "8_NHYb-fQu3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalaciones e Importaciones"
      ],
      "metadata": {
        "id": "iu4TosgXReRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías (si no están ya en Colab)\n",
        "!pip install nltk spacy > /dev/null\n",
        "!python -m spacy download es_core_news_sm > /dev/null # Modelo pequeño de español para spaCy"
      ],
      "metadata": {
        "id": "2CyxzGvIRdxg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import nltk\n",
        "import spacy\n",
        "import re # Para expresiones regulares (limpieza)\n",
        "import string # Para signos de puntuación"
      ],
      "metadata": {
        "id": "YNOnXGQsRrII"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "176_JS8VR2FR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390bc3b7-97fe-4d08-b399-199b5bdfe49b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo de spaCy en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "print(\"Modelo de spaCy 'es_core_news_sm' cargado.\")"
      ],
      "metadata": {
        "id": "2oqkr1o2SCab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44168df-a44d-42b9-a139-dbc3102ace55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de spaCy 'es_core_news_sm' cargado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar stopwords en español de NLTK\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')"
      ],
      "metadata": {
        "id": "RHvRsNlySKbH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de texto\n",
        "texto_ejemplo = \"Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\""
      ],
      "metadata": {
        "id": "MbQyV_bJlKHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:**\n",
        "\n",
        "Dado el siguiente conjunto de frases (reviews de películas):\n",
        "1.  Definir una función `preprocesar_nltk(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación).\n",
        "    *   Tokenice.\n",
        "    *   Quite stopwords (usando la lista de NLTK).\n",
        "    *   Aplique Stemming (con `SnowballStemmer`).\n",
        "    *   Devuelva la lista de stems.\n",
        "2.  Definir una función `preprocesar_spacy(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación - cuidado con no quitar espacios necesarios para spaCy).\n",
        "    *   Procese el texto con `nlp()`.\n",
        "    *   Devuelva la lista de lemas de los tokens que no sean stopwords (`token.is_stop`) y sean alfabéticos (`token.is_alpha`).\n",
        "3.  Aplicar ambas funciones a cada frase del dataset `reviews`.\n",
        "4.  Imprimir los resultados de ambas funciones para cada frase, uno debajo del otro, para poder comparar.\n",
        "5.  **Observar:** ¿Qué diferencias notables encuentran? ¿En qué casos un método parece funcionar \"mejor\" que el otro?"
      ],
      "metadata": {
        "id": "VMhtHWA5oovZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset para el ejercicio\n",
        "reviews = [\n",
        "    \"Una película emocionante con actuaciones brillantes. ¡Me encantó!\",\n",
        "    \"Muy aburrida y lenta. El guión era predecible y los actores no convencían.\",\n",
        "    \"Los efectos especiales fueron impresionantes, pero la historia dejaba mucho que desear.\",\n",
        "    \"¡Qué gran comedia! Me reí sin parar durante toda la película.\",\n",
        "    \"Un documental necesario que aborda temas importantes con profundidad y sensibilidad.\"\n",
        "]"
      ],
      "metadata": {
        "id": "57JZ3yh6oSDV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "def preprocesar_nltk(texto):\n",
        "    # 1. Minúsculas\n",
        "    texto = texto.lower()\n",
        "    # 2. Quitar números (usando expresiones regulares)\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    # 3. Quitar puntuación\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "    # 4. Quitar espacios extra\n",
        "    texto = texto.strip()\n",
        "    # 5. Tokenizar\n",
        "    tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "    # 6. Quitar stopwords\n",
        "    tokens_limpios = [palabra for palabra in tokens if palabra not in stopwords_es]\n",
        "\n",
        "    stems_nltk = [stemmer.stem(token) for token in tokens_limpios]\n",
        "\n",
        "    return \"Stems resultantes (NLTK): \", stems_nltk"
      ],
      "metadata": {
        "id": "vOBoHZAudgpj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesar_spacy(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación (dejamos espacios)\n",
        "  texto = texto.translate(str.maketrans(string.punctuation + '¡¿', ' ' * len(string.punctuation + '¡¿')))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "  doc = nlp(texto)\n",
        "  lemas_spacy = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "  return \"Lemas resultantes (spaCy): \", lemas_spacy"
      ],
      "metadata": {
        "id": "AWFmkVuLgEiv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stems_nltk = [preprocesar_nltk(review) for review in reviews]\n",
        "stems_spacy = [preprocesar_spacy(review) for review in reviews]\n",
        "\n",
        "print(stems_nltk)\n",
        "print(stems_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-pJn3HHg7KV",
        "outputId": "d5ca0240-dfcc-4665-ff84-6ae5ef17921f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Stems resultantes (NLTK): ', ['pelicul', 'emocion', 'actuacion', 'brillant', 'encant']), ('Stems resultantes (NLTK): ', ['aburr', 'lent', 'guion', 'predec', 'actor', 'convenc']), ('Stems resultantes (NLTK): ', ['efect', 'especial', 'impresion', 'histori', 'dej', 'des']), ('Stems resultantes (NLTK): ', ['gran', 'comedi', 'rei', 'par', 'tod', 'pelicul']), ('Stems resultantes (NLTK): ', ['documental', 'necesari', 'abord', 'tem', 'import', 'profund', 'sensibil'])]\n",
            "[('Lemas resultantes (spaCy): ', ['película', 'emocionante', 'actuación', 'brillante', 'encantar']), ('Lemas resultantes (spaCy): ', ['aburrido', 'lento', 'guión', 'predecible', 'actor', 'convencer']), ('Lemas resultantes (spaCy): ', ['efecto', 'especial', 'impresionante', 'historia', 'dejar', 'desear']), ('Lemas resultantes (spaCy): ', ['comedia', 'reí', 'parar', 'película']), ('Lemas resultantes (spaCy): ', ['documental', 'necesario', 'abordar', 'tema', 'importante', 'profundidad', 'sensibilidad'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: NLTK tiende a cortar las palabras mientras que Spacy interpreta prefectamente."
      ],
      "metadata": {
        "id": "csYnLkUDhPzY"
      }
    }
  ]
}